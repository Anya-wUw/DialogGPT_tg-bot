# DialogGPT_tg-bot
Dialog Telegram Bot based on ruDialoGPT<br><br>
  
Ссылка на Colab (с подробным описанием модели): https://colab.research.google.com/drive/13yY4eisgdwxYhLVZOpcMz-iy2R1xSn-O?usp=sharing<br>
Image Docker: ```docker pull anniborri/my_tg_bot``` <br>

---FT_DialogModel.ipynb - ноутбук с пояснениями к коду (скачен с коллаба) <br>
---bot.py - код для телеграмм-бота <br>
---prepare_messages.py - парсер телеграмм-чатов <br>

### Описание задачи
Обучение диалоговой языковой модели и интегрирование её с телеграм-ботом, а также оборачивание в докер.

### Загрузка данных
Исходные данные были сгенерированы вручную в телеграмме, в виде переписки между двумя пользователями. Этот шаг был выполнен, чтобы модель могла генерировать наиболее приемлемые ответы, так как тесты с другими открытыми чатами не дали приемлемого качества** модели (ограничения в колабе не дали обучить большой чат).

### Подготовка данных
Я создала словарь, состоящий из двух актеров: ввода пользователя (@@ПЕРВЫЙ@@) и ответа бота (@@ВТОРОЙ@@), что позволило мне подготовить тренировочный датасет для файтьюнинга.

### Гипотеза
Работая с колонкой reply_to_msg_id в датафрейме, я предположила, что если значение этой колонки равно -1, это означает начало новой темы в переписке. Все последующие сообщения с этим msg_id будут ответами на исходное сообщение. (Упрощенная гипотеза позволила быстро обучить модель на приемлемом качестве**)

### Fine-tuning модели
Для fine-tuning'а я использовала библиотеку Transformers и PyTorch. В качестве базовой модели была взята tinkoff-ai/ruDialoGPT-medium. Модель файтьюнилась на предобработанных данных с использованием оптимизатора AdamW с learning rate 1e-5 и размером батча 2 (из-за ограниченных ресурсов колаба был взят маленький батч).

### Тестирование модели
После обучения я провела базовое тестирование модели, отправив ей различные текстовые запросы и получив приемлемые ответы.

### Интеграция с Телеграм-ботом
Модель была интегрирована с телеграм-ботом для генерации ответов в реальном времени и обернута в докер-контейнер.

## Заключение
До файтьюнинга Телеграмм-бот не мог ответить логично на базовые промты например "привет)" или "как дела?". После файтьюнинга было замечено значительное улучшение ответов.

**Векторы для улучшения модели:**
1. **Расширение датасета для fine-tuning:**
Чем больше разнообразных историй переписок, тем лучше.
2. **Вложенная структура истории переписок:**
В текущей реализации есть только одно предыдущее сообщение в истории переписки. Можно рассмотреть более глубокую историю, где бот будет иметь доступ к нескольким предыдущим сообщениям пользователя. Это может улучшить обучение и сделать диалоги более качественными.
3. **Гиперпараметры обучения:** (batch_size, learning rate (AdamW), num_epochs) играют важную роль в обучении модели. Проведя больше экспериментов, можно найти оптимальные параметры, тем самым добиться лучших результатов

--------------
качество** модели было оценено субъективно
