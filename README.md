# DialogGPT_tg-bot
Dialog Telegram Bot based on tinkoff-ai/ruDialoGPT-medium<br><br>
  
Ссылка на Colab (с подробным описанием модели): https://colab.research.google.com/drive/13yY4eisgdwxYhLVZOpcMz-iy2R1xSn-O?usp=sharing<br>
Image Docker: ```docker pull anniborri/my_tg_bot``` <br>

* FT_DialogModel.ipynb - ноутбук с пояснениями к коду (скачен с коллаба) <br>
* bot.py - код для телеграмм-бота <br>
* prepare_messages.py - парсер телеграмм-чатов (```python prepare_messages.py result.json```)<br>

### Описание задачи
Обучение диалоговой языковой модели и интегрирование её с телеграм-ботом, а также оборачивание в докер.

### Загрузка данных
Исходные данные были сгенерированы вручную в телеграмме, в виде переписки между двумя пользователями. Этот шаг был выполнен, чтобы модель могла генерировать наиболее приемлемые ответы, так как тесты с другими открытыми чатами не дали приемлемого качества** модели (ограничения в колабе не дали обучить большой чат).

### Подготовка данных
Я создала словарь, состоящий из двух актеров: ввода пользователя (@@ПЕРВЫЙ@@) и ответа бота (@@ВТОРОЙ@@), что позволило мне подготовить тренировочный датасет для файтьюнинга. (подробнее в FT_DialogModel.ipynb)

### Гипотеза
Работая с колонкой reply_to_msg_id в датафрейме, я предположила, что если значение этой колонки равно -1, это означает начало новой темы в переписке. Все последующие сообщения с этим msg_id будут ответами на исходное сообщение. (Упрощенная гипотеза позволила быстро обучить модель на приемлемом качестве**) (подробнее в FT_DialogModel.ipynb)

### Fine-tuning модели
Для файнтьюнинга я использовала библиотеку Transformers и PyTorch. В качестве базовой модели была взята tinkoff-ai/ruDialoGPT-medium. Модель файтьюнилась на предобработанных данных с использованием оптимизатора AdamW с learning rate 1e-5 и размером батча 2 (из-за ограниченных ресурсов колаба был взят маленький батч). (подробнее в FT_DialogModel.ipynb)

### Тестирование модели
После обучения я провела базовое тестирование модели, отправив ей различные текстовые запросы и получив приемлемые ответы. (подробнее в FT_DialogModel.ipynb)

### Интеграция
Модель была интегрирована с телеграм-ботом для генерации ответов в реальном времени и обернута в докер-контейнер. (bot.py)
<br><br>
качество** модели было оценено субъективно

## Заключение
До файтьюнинга Телеграмм-бот не мог ответить логично на базовые промты например "привет)", "как дела?" или "что делаешь?". (см. фото ниже) <br>
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/44f22845-ea6b-4385-b006-bb2f72407492)
<br>
После файтьюнинга было замечено значительное улучшение ответов. (подробнее в FT_DialogModel.ipynb)

**Векторы для улучшения модели:**
1. **Расширение датасета для fine-tuning:**
Чем больше разнообразных историй переписок, тем лучше.
2. **Вложенная структура истории переписок:**
В текущей реализации есть только одно предыдущее сообщение в истории переписки. Можно рассмотреть более глубокую историю, где бот будет иметь доступ к нескольким предыдущим сообщениям пользователя. Это может улучшить обучение и сделать диалоги более качественными.
3. **Гиперпараметры обучения:** (batch_size, learning rate (AdamW), num_epochs) играют важную роль в обучении модели. Проведя больше экспериментов, можно найти оптимальные параметры, тем самым добиться лучших результатов

Пример работы модели: <br>
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/30cce450-6250-4b0d-9e18-046b40d7b928)
