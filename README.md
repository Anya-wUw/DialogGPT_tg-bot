# DialogGPT_tg-bot  

A Dialog Telegram Bot based on `tinkoff-ai/ruDialoGPT-medium`, trained and fine-tuned to generate human-like responses. This repository includes the code for fine-tuning the model, integrating it with a Telegram bot, and wrapping it in a Docker container for easy deployment.

---

## üìÑ Description  

This project demonstrates how to build a dialog model using **ruDialoGPT-medium**, fine-tune it with custom Telegram chat data, and deploy it as a functional Telegram bot. The model generates contextual responses by predicting the next turn in a conversation.  

**Fine-tuning was done with a custom dataset**, representing dialogues between two actors: the user (`@@FIRST@@`) and the bot (`@@SECOND@@`), which improved the model's performance in generating relevant responses.

---

## üîß Features  
- Fine-tuned `tinkoff-ai/ruDialoGPT-medium` for personalized dialog generation.  
- Real-time Telegram bot integration (`bot.py`).  
- Docker container for simplified deployment.  
- Preprocessing of Telegram chat data (`prepare_messages.py`).  
- Code explanation and training details in `FT_DialogModel.ipynb`.  

---

## üìÅ File Structure  

- **FT_DialogModel.ipynb**: Jupyter notebook with detailed code explanations for model fine-tuning and testing.  
- **bot.py**: Python script to integrate the fine-tuned model with Telegram's Bot API.  
- **prepare_messages.py**: Script for parsing Telegram chat data and preparing it for training (`python prepare_messages.py result.json`).  

---

## üöÄ Fine-tuning Process  

The fine-tuning was done using **Transformers** and **PyTorch** libraries. The base model, `tinkoff-ai/ruDialoGPT-medium`, was trained on manually created Telegram chat data to generate more coherent responses. Key steps:  

1. **Preprocessing**: Data was preprocessed into user-bot pairs.  
2. **Fine-tuning**: The model was fine-tuned using the AdamW optimizer (`learning_rate = 1e-5`, batch size = 2).  
3. **Testing**: The model's responses were evaluated on various text prompts, showing significant improvements after fine-tuning.  

---

## üõ†Ô∏è Requirements  

- Python 3.7+  
- PyTorch  
- Transformers  
- Telegram Bot API  
- Docker  

---

## üì¶ Installation & Usage  

1. Clone the repository:  
   ```bash
   git clone https://github.com/your-repo/DialogGPT_tg-bot.git
   cd DialogGPT_tg-bot
   ```

2. Build and run the Docker container:  
   ```bash
   docker pull anniborri/my_tg_bot
   docker run -it anniborri/my_tg_bot
   ```

3. Set up your Telegram bot token in `bot.py`.  
4. Run the bot:  
   ```bash
   python bot.py
   ```

---

## üîç Example  

Before fine-tuning:  
The bot couldn‚Äôt respond logically to basic prompts like **"Hello"**, **"How are you?"**, or **"What are you doing?"**.  

After fine-tuning:  
The responses became much more coherent and relevant, showing significant improvements.  

Example response:  
![Example Output](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/30cce450-6250-4b0d-9e18-046b40d7b928)

---

## üìà Model Improvements  

1. **Expand the Dataset**: Use more diverse conversation datasets for better response quality.  
2. **Include Nested Conversation History**: Enable the bot to access multiple previous messages for context.  
3. **Optimize Training Hyperparameters**: Experiment with different batch sizes, learning rates, and epochs for improved results.  

<br><hr><br>
(Russian Translation)
# DialogGPT_tg-bot
Dialog Telegram Bot based on tinkoff-ai/ruDialoGPT-medium<br><br>
  
–°—Å—ã–ª–∫–∞ –Ω–∞ Colab (—Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏): https://colab.research.google.com/drive/13yY4eisgdwxYhLVZOpcMz-iy2R1xSn-O?usp=sharing<br>
Image Docker: ```docker pull anniborri/my_tg_bot``` <br>

* FT_DialogModel.ipynb - –Ω–æ—É—Ç–±—É–∫ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ –∫ –∫–æ–¥—É (—Å–∫–∞—á–µ–Ω —Å –∫–æ–ª–ª–∞–±–∞) <br>
* bot.py - –∫–æ–¥ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º–º-–±–æ—Ç–∞ <br>
* prepare_messages.py - –ø–∞—Ä—Å–µ—Ä —Ç–µ–ª–µ–≥—Ä–∞–º–º-—á–∞—Ç–æ–≤ (```python prepare_messages.py result.json```)<br>

### –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
–û–±—É—á–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—ë —Å —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–æ–º, –∞ —Ç–∞–∫–∂–µ –æ–±–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –≤ –¥–æ–∫–µ—Ä.

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤—Ä—É—á–Ω—É—é –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–º–µ, –≤ –≤–∏–¥–µ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –≠—Ç–æ—Ç —à–∞–≥ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–∏–µ–º–ª–µ–º—ã–µ –æ—Ç–≤–µ—Ç—ã, —Ç–∞–∫ –∫–∞–∫ —Ç–µ—Å—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ —á–∞—Ç–∞–º–∏ –Ω–µ –¥–∞–ª–∏ –ø—Ä–∏–µ–º–ª–µ–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞** –º–æ–¥–µ–ª–∏ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –∫–æ–ª–∞–±–µ –Ω–µ –¥–∞–ª–∏ –æ–±—É—á–∏—Ç—å –±–æ–ª—å—à–æ–π —á–∞—Ç).
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/bdfa799a-708c-4af3-be75-059d3d99d71d)


### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
–Ø —Å–æ–∑–¥–∞–ª–∞ —Å–ª–æ–≤–∞—Ä—å, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –¥–≤—É—Ö –∞–∫—Ç–µ—Ä–æ–≤: –≤–≤–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (@@–ü–ï–†–í–´–ô@@) –∏ –æ—Ç–≤–µ—Ç–∞ –±–æ—Ç–∞ (@@–í–¢–û–†–û–ô@@), —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –º–Ω–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∞–π—Ç—å—é–Ω–∏–Ω–≥–∞. (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ FT_DialogModel.ipynb)
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/68afe01b-a4e2-4c2c-9b2e-e993518d6abd)
<br>
### –ì–∏–ø–æ—Ç–µ–∑–∞
–†–∞–±–æ—Ç–∞—è —Å –∫–æ–ª–æ–Ω–∫–æ–π reply_to_msg_id –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ, —è –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–ª–∞, —á—Ç–æ –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ —ç—Ç–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Ä–∞–≤–Ω–æ -1, —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —Ç–µ–º—ã –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ. –í—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å —ç—Ç–∏–º msg_id –±—É–¥—É—Ç –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–∑–≤–æ–ª–∏–ª–∞ –±—ã—Å—Ç—Ä–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–µ–º–ª–µ–º–æ–º –∫–∞—á–µ—Å—Ç–≤–µ**) (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ FT_DialogModel.ipynb)

### Fine-tuning –º–æ–¥–µ–ª–∏
–î–ª—è —Ñ–∞–π–Ω—Ç—å—é–Ω–∏–Ω–≥–∞ —è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫—É Transformers –∏ PyTorch. –í –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ –≤–∑—è—Ç–∞ tinkoff-ai/ruDialoGPT-medium. –ú–æ–¥–µ–ª—å —Ñ–∞–π—Ç—å—é–Ω–∏–ª–∞—Å—å –Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ AdamW —Å learning rate 1e-5 –∏ —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ 2 (–∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∫–æ–ª–∞–±–∞ –±—ã–ª –≤–∑—è—Ç –º–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á). –ó–∞—Ç–µ–º –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ—Ç–æ–º. (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ FT_DialogModel.ipynb)

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —è –ø—Ä–æ–≤–µ–ª–∞ –±–∞–∑–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ—Ç–ø—Ä–∞–≤–∏–≤ –µ–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –ø–æ–ª—É—á–∏–≤ –ø—Ä–∏–µ–º–ª–µ–º—ã–µ –æ—Ç–≤–µ—Ç—ã, –ø—Ä–∏—Å—Ç—É–ø–∏–ª–∞ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏. (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ FT_DialogModel.ipynb)

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
–ú–æ–¥–µ–ª—å –±—ã–ª–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ –¥–æ–∫–µ—Ä-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä. (bot.py)
<br><br>
–∫–∞—á–µ—Å—Ç–≤–æ** –º–æ–¥–µ–ª–∏ –±—ã–ª–æ –æ—Ü–µ–Ω–µ–Ω–æ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–î–æ —Ñ–∞–π—Ç—å—é–Ω–∏–Ω–≥–∞ –¢–µ–ª–µ–≥—Ä–∞–º–º-–±–æ—Ç –Ω–µ –º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å –ª–æ–≥–∏—á–Ω–æ –Ω–∞ –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ–º—Ç—ã –Ω–∞–ø—Ä–∏–º–µ—Ä "–ø—Ä–∏–≤–µ—Ç)", "–∫–∞–∫ –¥–µ–ª–∞?" –∏–ª–∏ "—á—Ç–æ –¥–µ–ª–∞–µ—à—å?". (—Å–º. —Ñ–æ—Ç–æ –Ω–∏–∂–µ) <br>
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/44f22845-ea6b-4385-b006-bb2f72407492)
<br>
–ü–æ—Å–ª–µ —Ñ–∞–π—Ç—å—é–Ω–∏–Ω–≥–∞ –±—ã–ª–æ –∑–∞–º–µ—á–µ–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤. (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ FT_DialogModel.ipynb)

**–í–µ–∫—Ç–æ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏:**
1. **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è fine-tuning:**
–ß–µ–º –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π –ø–µ—Ä–µ–ø–∏—Å–æ–∫, —Ç–µ–º –ª—É—á—à–µ.
2. **–í–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏ –ø–µ—Ä–µ–ø–∏—Å–æ–∫:**
–í —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ú–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫—É—é –∏—Å—Ç–æ—Ä–∏—é, –≥–¥–µ –±–æ—Ç –±—É–¥–µ—Ç –∏–º–µ—Ç—å –¥–æ—Å—Ç—É–ø –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–æ–æ–±—â–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –≠—Ç–æ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–¥–µ–ª–∞—Ç—å –¥–∏–∞–ª–æ–≥–∏ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏.
3. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:** (batch_size, learning rate (AdamW), num_epochs) –∏–≥—Ä–∞—é—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ–¥—è –±–æ–ª—å—à–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ç–µ–º —Å–∞–º—ã–º –¥–æ–±–∏—Ç—å—Å—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏: <br>
![image](https://github.com/Anya-wUw/DialogGPT_tg-bot/assets/48104500/30cce450-6250-4b0d-9e18-046b40d7b928)
